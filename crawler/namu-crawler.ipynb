{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "scrapingenv1",
   "display_name": "scrapingEnv1",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 나무위키 크롤러 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. 나무위키 구조 정의\n",
    "\n",
    "<p>\n",
    "    나무위키 컨탠츠 구조<br>\n",
    "    소제목 : h2(class:wiki-heading) -> span(id=소제목)<br>\n",
    "    내용 : div(class:wiki-heading-content) -> div(class:wiki-paragraph)\n",
    "</p>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "req = requests.get('https://namu.wiki/w/테스터훈')\n",
    "bs = BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "targets = bs.find_all(\"h2\", class_=\"wiki-heading\")\n",
    "for target in targets:\n",
    "    realtarget = target.next_sibling.get_text()\n",
    "    print(realtarget)\n",
    "\n",
    "targets = bs.find_all(\"h3\", class_=\"wiki-heading\")\n",
    "for target in targets:\n",
    "    realtarget = target.next_sibling.get_text()\n",
    "    print(realtarget)\n",
    "\n",
    "targets = bs.find_all(\"h4\", class_=\"wiki-heading\")\n",
    "for target in targets:\n",
    "    realtarget = target.next_sibling.get_text()\n",
    "    print(realtarget)\n"
   ]
  },
  {
   "source": [
    "## 2. 크롤러 클래스 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#크롤러\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Namu_Crawler:\n",
    "    def __init__(self, keyword):\n",
    "        self.keyword = keyword\n",
    "\n",
    "    def getPage(self):\n",
    "        try:\n",
    "            req = requests.get('https://namu.wiki/w/'+self.keyword)\n",
    "        except requests.exceptions.RequestException:\n",
    "            print('request error')\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "    def crawl(self):\n",
    "        youtuberstring = []\n",
    "        bs = self.getPage()\n",
    "        targets = bs.find_all(\"h2\", class_=\"wiki-heading\")\n",
    "        for target in targets:\n",
    "            youtuberstring.append(target.next_sibling.get_text())\n",
    "\n",
    "        targets = bs.find_all(\"h3\", class_=\"wiki-heading\")\n",
    "        for target in targets:\n",
    "            youtuberstring.append(target.next_sibling.get_text())\n",
    "\n",
    "        targets = bs.find_all(\"h4\", class_=\"wiki-heading\")\n",
    "        for target in targets:\n",
    "            youtuberstring.append(target.next_sibling.get_text())\n",
    "\n",
    "        return youtuberstring\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## 3. 크롤링 대상"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#상위 50명 분석\n",
    "gameyoutubers = ['도티','잠뜰','감스트','김블루','대도서관','양띵','김재원(샌드박스%20네트워크)','뜨뜨뜨뜨','태경','악어(인터넷%20방송인)','테스터훈','우주하마','우왁굳','마이린','릴카','겜브링','킴성태','테드%20TV','쁘허','기리TV','캐릭온','텔론','램램','마재','makeUmove','연다','울산큰고래','머독','혜안(유튜버)','잉여맨','군림보','과로사','침착맨','김왼팔','Cowsep','돼지저금통(유튜버)','괴물쥐','만두민','문호준','강지','개리형','로이조','삼식']\n",
    "\n",
    "strings = \"\"\n",
    "for youtuber in gameyoutubers:\n",
    "    crawler = Namu_Crawler(youtuber)\n",
    "    one_string = \"\".join(crawler.crawl())\n",
    "    strings += \" \" + one_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1인 분석시\n",
    "crawler = Namu_Crawler('테스터훈')\n",
    "strings = \" \".join(crawler.crawl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 파일 추출\n",
    "\n",
    "f = open('./crawling_result_text/testerhoon_namu.txt', mode='wt', encoding='utf-8')\n",
    "\n",
    "f.write(strings)\n",
    "f.close()"
   ]
  },
  {
   "source": [
    "## 4. 불용어 사전"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어사전(자체 제작)\n",
    "stopwords = ['라', '중', '이', '때문', '지', '이상', '등', '수', '것', '시작', '부분', '당시', '경우', '이후', '오브', '리그', '편', '위', '정도', '활동', '전', \n",
    "    '한', '도중', '자체', '경기', '방송이','때','녹두','이유','문단을','대부분','번','일','리','두','초','나','문단','문','방송','적','후','단어','게임','유튜브','영상','말','역사','시청자들','본인','이전','시청자','2020년','2018년','2019년','2017년','2016년','업로드','콘텐츠','듯','현재','명','들','사람','1','개','관련','채널','모습','주','거','내','사이','자신','유튜버','컨텐츠','생각']  + ['1월','2월','3월','4월','5월','6월','7월','8월','9월','10월','11월','12월'] + ['테스터훈','한동숙'] #+ gameyoutubers"
   ]
  },
  {
   "source": [
    "## 5. 워드 클라우드 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#워드클라우드\n",
    "# -*- encoding:utf8 -*-\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "from konlpy.tag import Hannanum\n",
    "import pytagcloud\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "r = lambda: random.randint(0,255)\n",
    "color = lambda: (r(),r(),r())\n",
    "\n",
    "def get_tags(text, ntags=50, multiplier=3):\n",
    "    h = Hannanum()\n",
    "    nouns = h.nouns(text)\n",
    "    unique_nouns = set(nouns)\n",
    "    for word in unique_nouns:\n",
    "        if word in stopwords:\n",
    "            while word in nouns:\n",
    "                nouns.remove(word)\n",
    "\n",
    "\n",
    "\n",
    "    count = Counter(nouns)\n",
    "    return [{ 'color': color(), 'tag': n, 'size': int(c*multiplier) }\\\n",
    "                for n, c in count.most_common(ntags)]\n",
    "                \n",
    "\n",
    "def draw_cloud(tags, filename, fontname='Korean', size=(800, 600)):\n",
    "    pytagcloud.create_tag_image(tags, filename, fontname=fontname, size=size)\n",
    "    webbrowser.open( os.path.relpath(filename) )\n",
    "\n",
    "\n",
    "tags = get_tags(strings)\n",
    "print(tags)\n",
    "draw_cloud(tags,'./crawling_result_wordcloud/WC_handongsuk.png')\n",
    "\n"
   ]
  }
 ]
}