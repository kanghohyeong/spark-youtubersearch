{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled57.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yMv45hk_AXt",
        "outputId": "d8f9e3c4-f269-4c3e-e258-9c973b4c9372"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.4)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApP_RjbZ-czZ"
      },
      "source": [
        "from konlpy.tag import *\r\n",
        "komoran = Komoran()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZsJsF7ozKk-",
        "outputId": "b1c4cc3e-b167-4e3e-a511-413dc4c9fee1"
      },
      "source": [
        "!pip install transformers boto3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.16.48)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.48 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.19.48)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.48->boto3) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CABfICCvzOMM",
        "outputId": "c65009ca-047c-4b08-812e-d196f631dcaf"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "import torch\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from transformers import BertConfig , BertForSequenceClassification\r\n",
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained('beomi/kcbert-base')\r\n",
        "tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKyJ3Wtucq-8"
      },
      "source": [
        "def split_token_get(text,tokenizer,MAX):\r\n",
        "  split_tokens = []\r\n",
        "  for token in komoran.morphs(text):\r\n",
        "    for sub_token in tokenizer.tokenize(token):\r\n",
        "      split_tokens.append(sub_token)\r\n",
        "  if len(split_tokens) > MAX-2:\r\n",
        "    split_tokens = split_tokens[:(MAX-2)]\r\n",
        "  split_tokens = [\"[CLS]\"]+ split_tokens + [\"[SEP]\"]\r\n",
        "  return split_tokens\r\n",
        "def get_input_ids(token , tokenizer , MAX):\r\n",
        "  segment_ids = [0] * len(token)\r\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(token)\r\n",
        "\r\n",
        "  input_mask = [1] * (len(input_ids) -1 )\r\n",
        "  input_mask += [0]\r\n",
        "  \r\n",
        "  padding = [0] * (MAX -len(input_ids) )\r\n",
        "\r\n",
        "  input_ids += padding\r\n",
        "  input_mask += padding\r\n",
        "  segment_ids += padding\r\n",
        "\r\n",
        "  assert len(input_ids) == MAX\r\n",
        "  return input_ids \r\n",
        "def get_segment_ids(token, tokenizer, MAX):\r\n",
        "  segment_ids = [0] * len(token)\r\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(token)\r\n",
        "  padding = [0] * (MAX -len(input_ids) )\r\n",
        "  \r\n",
        "  segment_ids += padding\r\n",
        "  assert len(segment_ids) == MAX\r\n",
        "  return segment_ids \r\n",
        "def get_input_mask(token, tokenizer, MAX):\r\n",
        "  \r\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(token)\r\n",
        "\r\n",
        "  input_mask = [1]* (len(input_ids) -1 )\r\n",
        "  input_mask += [0]\r\n",
        "\r\n",
        "  padding = [0] * (MAX -len(input_ids) )\r\n",
        "\r\n",
        "  input_mask += padding\r\n",
        "\r\n",
        "  assert len(input_mask) == MAX\r\n",
        "  return input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTtuQ8GizbIS"
      },
      "source": [
        "data = pd.read_excel('실력긍부정데이터수집.xlsx')\r\n",
        "data = data.sample(frac=1)\r\n",
        "\r\n",
        "sentences = data['sentence']\r\n",
        "labels = data['lable'].values\r\n",
        "\r\n",
        "\r\n",
        "sentences1 = sentences[299:]\r\n",
        "labels1 = labels[299:]\r\n",
        "\r\n",
        "sentences = sentences[:299]\r\n",
        "labels = labels[:299]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EBS03xOzmkk"
      },
      "source": [
        "data2 = pd.read_csv(\"ability_sentences.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x14KtW10Jqd"
      },
      "source": [
        "sentences1 = data['sentence']\r\n",
        "tokenized_texts = [split_token_get(ss, tokenizer,128) for ss in sentences1]\r\n",
        "input_ids = [get_input_ids(ss,tokenizer,128)for ss in tokenized_texts]\r\n",
        "segment_ids = [get_segment_ids(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "input_mask = [get_input_mask(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "\r\n",
        "input_ids = torch.tensor(input_ids)\r\n",
        "segment_ids = torch.tensor(segment_ids)\r\n",
        "input_mask = torch.tensor(input_mask)\r\n",
        "labels = torch.tensor(labels1)\r\n",
        "\r\n",
        "train_data1 = TensorDataset(input_ids,input_mask)\r\n",
        "train_dataloader1 = DataLoader(train_data1, batch_size = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s2gR5s6ztva"
      },
      "source": [
        "tokenized_texts = [split_token_get(ss, tokenizer,128) for ss in sentences]\r\n",
        "input_ids = [get_input_ids(ss,tokenizer,128)for ss in tokenized_texts]\r\n",
        "segment_ids = [get_segment_ids(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "input_mask = [get_input_mask(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "\r\n",
        "input_ids = torch.tensor(input_ids)\r\n",
        "segment_ids = torch.tensor(segment_ids)\r\n",
        "input_mask = torch.tensor(input_mask)\r\n",
        "labels = torch.tensor(labels)\r\n",
        "\r\n",
        "train_data = TensorDataset(input_ids,input_mask,labels)\r\n",
        "train_dataloader = DataLoader(train_data, batch_size = 8)\r\n",
        "\r\n",
        "\r\n",
        "tokenized_texts = [split_token_get(ss, tokenizer,128) for ss in sentences1]\r\n",
        "input_ids = [get_input_ids(ss,tokenizer,128)for ss in tokenized_texts]\r\n",
        "segment_ids = [get_segment_ids(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "input_mask = [get_input_mask(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "\r\n",
        "input_ids = torch.tensor(input_ids)\r\n",
        "segment_ids = torch.tensor(segment_ids)\r\n",
        "input_mask = torch.tensor(input_mask)\r\n",
        "labels = torch.tensor(labels1)\r\n",
        "\r\n",
        "train_data1 = TensorDataset(input_ids,input_mask,labels)\r\n",
        "train_dataloader1 = DataLoader(train_data1, batch_size = 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yh0Cplizx-n",
        "outputId": "e0d221e7-7747-4ec5-bde6-4c0ec8766713"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')\r\n",
        "if torch.cuda.is_available():    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(300, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4dasRSoz0-y"
      },
      "source": [
        "import numpy as np\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    \r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "def format_time(elapsed):\r\n",
        "    # 반올림\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # hh:mm:ss으로 형태 변경\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbKpLXlZVpEc",
        "outputId": "17ecc4d0-a1f3-4c83-c0e5-64f690ba33e7"
      },
      "source": [
        "from transformers import AdamW\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  lr = 2e-5,\r\n",
        "                  eps = 1e-8)\r\n",
        "epochs = 40\r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps = 0,\r\n",
        "                                            num_training_steps = total_steps)\r\n",
        "\r\n",
        "seed_val = 42\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "model.zero_grad()\r\n",
        "eval_accuracy = 0\r\n",
        "nb_eval_steps = 0\r\n",
        "for i in range(0, epochs):\r\n",
        "  optimizer.zero_grad()\r\n",
        "  t0 = time.time()\r\n",
        "  total_loss = 0\r\n",
        "  model.train()\r\n",
        "  eval_accuracy = 0\r\n",
        "  nb_eval_steps = 0\r\n",
        "  for step,batch in enumerate(train_dataloader):\r\n",
        "    model.train()\r\n",
        "    if step%100 == 0 and not step ==0:\r\n",
        "      elapsed = format_time(time.time() - t0)\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed) )\r\n",
        "    \r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "    input_ids , input_mask , labels = batch\r\n",
        "\r\n",
        "    outputs = model(input_ids= input_ids,\r\n",
        "                    token_type_ids = None,\r\n",
        "                    attention_mask=None,\r\n",
        "                    labels = labels)\r\n",
        "    loss = outputs\r\n",
        "\r\n",
        "    total_loss += loss[0].item()\r\n",
        "\r\n",
        "    loss[0].backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "  \r\n",
        "    optimizer.step()\r\n",
        "    scheduler.step()\r\n",
        "\r\n",
        "    model.zero_grad()\r\n",
        "  avg_train_loss = total_loss / len(train_dataloader)            \r\n",
        "\r\n",
        "  print(\"\")\r\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "  print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "  \r\n",
        "  t0 = time.time()\r\n",
        "  model.eval()\r\n",
        "  \r\n",
        "  eval_accuracy = 0\r\n",
        "  nb_eval_steps = 0\r\n",
        "  for step,batch in enumerate(train_dataloader1):\r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "\r\n",
        "    input_ids , input_mask, labels = batch\r\n",
        "    outputs = model(input_ids = input_ids ,\r\n",
        "                    token_type_ids = None , \r\n",
        "                    attention_mask = input_mask \r\n",
        "                    )\r\n",
        "    logits = outputs[0]\r\n",
        "\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "    labels = labels.to('cpu').numpy()\r\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, labels)\r\n",
        "    eval_accuracy += tmp_eval_accuracy\r\n",
        "    nb_eval_steps += 1\r\n",
        "  print(\"예측 Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\r\n",
        "  print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:00:09\n",
            "욕설 예측도 Accuracy: 0.65385\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epcoh took: 0:00:09\n",
            "욕설 예측도 Accuracy: 0.76923\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:00:09\n",
            "욕설 예측도 Accuracy: 0.75000\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:00:09\n",
            "욕설 예측도 Accuracy: 0.81731\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.75962\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.77885\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.77885\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.77885\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.81731\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.81731\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.81731\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.80769\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.82692\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:00:08\n",
            "욕설 예측도 Accuracy: 0.79808\n",
            "  Validation took: 0:00:01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiSwz3B_z5oe"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/recent_model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdAGt0NK1C5y"
      },
      "source": [
        "def softmax(a):\r\n",
        "  c = np.max(a)\r\n",
        "  exp_a = np.exp(a-c)\r\n",
        "  sum_exp_a = np.sum(exp_a)\r\n",
        "  y = exp_a / sum_exp_a\r\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojOXLMsZzih4",
        "outputId": "8ede5185-a58c-441b-af48-e5cbe03c8cb7"
      },
      "source": [
        "model.eval()\r\n",
        "for step,batch in enumerate(train_dataloader1):\r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "\r\n",
        "    input_ids , input_mask = batch\r\n",
        "    outputs = model(input_ids = input_ids ,\r\n",
        "                    token_type_ids = None , \r\n",
        "                    attention_mask = input_mask \r\n",
        "                    )\r\n",
        "    logits = outputs[0]\r\n",
        "\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "    print(softmax(logits)[0])\r\n",
        "    data2['label'].loc[step] = round(softmax(logits)[0][1],4)\r\n",
        "    eval_accuracy += tmp_eval_accuracy\r\n",
        "    nb_eval_steps += 1\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.9995494e-01 4.5102839e-05]\n",
            "[5.510646e-05 9.999449e-01]\n",
            "[9.9997449e-01 2.5532549e-05]\n",
            "[1.8832194e-05 9.9998116e-01]\n",
            "[2.1312795e-05 9.9997866e-01]\n",
            "[2.5699588e-05 9.9997425e-01]\n",
            "[1.8389519e-05 9.9998164e-01]\n",
            "[1.9868465e-05 9.9998009e-01]\n",
            "[2.6187017e-05 9.9997377e-01]\n",
            "[1.7995648e-05 9.9998200e-01]\n",
            "[3.0864394e-04 9.9969137e-01]\n",
            "[2.2795495e-05 9.9997723e-01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[9.999236e-01 7.637066e-05]\n",
            "[6.4934284e-05 9.9993503e-01]\n",
            "[1.9976911e-05 9.9997997e-01]\n",
            "[3.7957605e-05 9.9996209e-01]\n",
            "[9.9989939e-01 1.0054953e-04]\n",
            "[2.6053478e-05 9.9997389e-01]\n",
            "[9.9989510e-01 1.0485207e-04]\n",
            "[9.7577693e-05 9.9990237e-01]\n",
            "[5.9898270e-05 9.9994016e-01]\n",
            "[1.8199804e-05 9.9998176e-01]\n",
            "[2.910575e-05 9.999709e-01]\n",
            "[1.9331931e-04 9.9980670e-01]\n",
            "[9.99936e-01 6.39796e-05]\n",
            "[9.9997163e-01 2.8371294e-05]\n",
            "[5.517639e-05 9.999448e-01]\n",
            "[2.916006e-05 9.999708e-01]\n",
            "[2.10250e-05 9.99979e-01]\n",
            "[2.4904168e-05 9.9997509e-01]\n",
            "[9.9995708e-01 4.2957683e-05]\n",
            "[2.4146895e-05 9.9997580e-01]\n",
            "[2.4339599e-05 9.9997568e-01]\n",
            "[3.6843532e-05 9.9996316e-01]\n",
            "[3.8556354e-05 9.9996150e-01]\n",
            "[2.3857050e-05 9.9997616e-01]\n",
            "[9.9994290e-01 5.7156958e-05]\n",
            "[2.1306898e-05 9.9997866e-01]\n",
            "[2.0084422e-05 9.9997997e-01]\n",
            "[9.9968696e-01 3.1303099e-04]\n",
            "[4.286642e-05 9.999571e-01]\n",
            "[3.2068605e-05 9.9996793e-01]\n",
            "[2.0625743e-05 9.9997938e-01]\n",
            "[4.9371133e-04 9.9950624e-01]\n",
            "[2.9411443e-05 9.9997056e-01]\n",
            "[9.9992275e-01 7.7227807e-05]\n",
            "[9.9990964e-01 9.0307927e-05]\n",
            "[2.5109590e-05 9.9997485e-01]\n",
            "[9.9996650e-01 3.3543587e-05]\n",
            "[3.596363e-05 9.999640e-01]\n",
            "[2.6296631e-05 9.9997365e-01]\n",
            "[3.2801214e-05 9.9996722e-01]\n",
            "[2.5857909e-05 9.9997413e-01]\n",
            "[9.9983919e-01 1.6084676e-04]\n",
            "[4.1821790e-05 9.9995816e-01]\n",
            "[0.99822634 0.00177361]\n",
            "[2.4474763e-05 9.9997556e-01]\n",
            "[2.0220758e-05 9.9997973e-01]\n",
            "[3.090489e-05 9.999691e-01]\n",
            "[4.3910808e-05 9.9995613e-01]\n",
            "[2.1089802e-05 9.9997890e-01]\n",
            "[4.1232252e-05 9.9995875e-01]\n",
            "[7.0159396e-05 9.9992979e-01]\n",
            "[9.9996579e-01 3.4249675e-05]\n",
            "[9.999083e-01 9.168868e-05]\n",
            "[2.486541e-05 9.999751e-01]\n",
            "[5.102098e-05 9.999490e-01]\n",
            "[8.323585e-05 9.999168e-01]\n",
            "[1.8097342e-05 9.9998188e-01]\n",
            "[9.9991035e-01 8.9677545e-05]\n",
            "[3.9959563e-05 9.9996006e-01]\n",
            "[2.0449315e-05 9.9997950e-01]\n",
            "[9.990717e-01 9.282712e-04]\n",
            "[2.3253378e-05 9.9997675e-01]\n",
            "[2.9262466e-05 9.9997079e-01]\n",
            "[1.3344621e-04 9.9986649e-01]\n",
            "[8.4067360e-05 9.9991596e-01]\n",
            "[9.999449e-01 5.502453e-05]\n",
            "[2.9947674e-05 9.9997008e-01]\n",
            "[9.9963975e-01 3.6019285e-04]\n",
            "[2.6408718e-05 9.9997354e-01]\n",
            "[9.9979347e-01 2.0656679e-04]\n",
            "[2.4399300e-05 9.9997556e-01]\n",
            "[0.00811775 0.99188226]\n",
            "[0.98314375 0.01685631]\n",
            "[9.9995613e-01 4.3900509e-05]\n",
            "[9.9993932e-01 6.0721835e-05]\n",
            "[9.9996579e-01 3.4214678e-05]\n",
            "[7.3550509e-05 9.9992645e-01]\n",
            "[4.0041308e-05 9.9995995e-01]\n",
            "[9.9980682e-01 1.9325297e-04]\n",
            "[8.058166e-05 9.999194e-01]\n",
            "[2.0965694e-05 9.9997902e-01]\n",
            "[3.2525608e-05 9.9996746e-01]\n",
            "[9.9975365e-01 2.4630752e-04]\n",
            "[2.8159566e-05 9.9997187e-01]\n",
            "[4.0048413e-05 9.9995995e-01]\n",
            "[0.00111705 0.99888295]\n",
            "[3.143694e-05 9.999685e-01]\n",
            "[9.9942434e-01 5.7570619e-04]\n",
            "[3.854952e-05 9.999615e-01]\n",
            "[0.00156528 0.9984347 ]\n",
            "[2.2104410e-05 9.9997795e-01]\n",
            "[1.3875846e-04 9.9986124e-01]\n",
            "[3.8875947e-05 9.9996114e-01]\n",
            "[2.5543166e-05 9.9997449e-01]\n",
            "[2.1236792e-05 9.9997878e-01]\n",
            "[2.1731856e-05 9.9997830e-01]\n",
            "[9.9992311e-01 7.6899465e-05]\n",
            "[1.440560e-04 9.998559e-01]\n",
            "[3.7399306e-05 9.9996257e-01]\n",
            "[2.2147497e-05 9.9997783e-01]\n",
            "[1.9385430e-05 9.9998057e-01]\n",
            "[2.9938936e-05 9.9997008e-01]\n",
            "[9.9989104e-01 1.0893742e-04]\n",
            "[0.99865866 0.00134138]\n",
            "[2.3855548e-05 9.9997616e-01]\n",
            "[1.5718877e-04 9.9984276e-01]\n",
            "[9.999492e-01 5.075737e-05]\n",
            "[9.9996829e-01 3.1677177e-05]\n",
            "[3.1354783e-05 9.9996865e-01]\n",
            "[9.9988580e-01 1.1421459e-04]\n",
            "[5.69816e-05 9.99943e-01]\n",
            "[9.9969125e-01 3.0875401e-04]\n",
            "[9.9995327e-01 4.6716210e-05]\n",
            "[6.946663e-05 9.999305e-01]\n",
            "[9.9988568e-01 1.1433719e-04]\n",
            "[5.9912432e-05 9.9994004e-01]\n",
            "[9.9969888e-01 3.0111056e-04]\n",
            "[1.2756027e-04 9.9987245e-01]\n",
            "[9.9997497e-01 2.5073794e-05]\n",
            "[9.9997556e-01 2.4396648e-05]\n",
            "[3.0194586e-05 9.9996984e-01]\n",
            "[9.9983764e-01 1.6230033e-04]\n",
            "[2.1593916e-05 9.9997842e-01]\n",
            "[3.7004171e-05 9.9996305e-01]\n",
            "[9.9909353e-01 9.0642663e-04]\n",
            "[2.224792e-05 9.999777e-01]\n",
            "[1.1800267e-04 9.9988198e-01]\n",
            "[3.7594731e-05 9.9996245e-01]\n",
            "[9.999062e-01 9.385915e-05]\n",
            "[2.4505082e-04 9.9975497e-01]\n",
            "[3.5140612e-05 9.9996483e-01]\n",
            "[5.0106199e-05 9.9994993e-01]\n",
            "[2.043908e-05 9.999796e-01]\n",
            "[2.5043944e-05 9.9997497e-01]\n",
            "[0.94580066 0.05419934]\n",
            "[9.9963534e-01 3.6468671e-04]\n",
            "[3.0159646e-05 9.9996984e-01]\n",
            "[4.2565804e-05 9.9995744e-01]\n",
            "[2.9456300e-05 9.9997056e-01]\n",
            "[1.9674915e-05 9.9998033e-01]\n",
            "[1.8101036e-05 9.9998188e-01]\n",
            "[3.245218e-05 9.999676e-01]\n",
            "[4.415440e-05 9.999559e-01]\n",
            "[9.9991035e-01 8.9619236e-05]\n",
            "[2.8185814e-05 9.9997187e-01]\n",
            "[1.8384328e-05 9.9998164e-01]\n",
            "[9.9994349e-01 5.6497498e-05]\n",
            "[4.1350933e-05 9.9995863e-01]\n",
            "[9.9976009e-01 2.3986677e-04]\n",
            "[2.7400263e-05 9.9997258e-01]\n",
            "[2.8216418e-05 9.9997175e-01]\n",
            "[9.9989069e-01 1.0930119e-04]\n",
            "[1.9892961e-05 9.9998009e-01]\n",
            "[6.8088833e-05 9.9993193e-01]\n",
            "[1.8235487e-05 9.9998176e-01]\n",
            "[2.3900091e-05 9.9997616e-01]\n",
            "[9.9994135e-01 5.8613892e-05]\n",
            "[1.8998058e-05 9.9998105e-01]\n",
            "[9.9993205e-01 6.7920351e-05]\n",
            "[9.9995422e-01 4.5811765e-05]\n",
            "[9.999535e-01 4.648796e-05]\n",
            "[1.8485734e-05 9.9998152e-01]\n",
            "[9.9983442e-01 1.6557872e-04]\n",
            "[2.1252392e-05 9.9997878e-01]\n",
            "[2.1370515e-05 9.9997866e-01]\n",
            "[3.9245235e-05 9.9996078e-01]\n",
            "[3.4054196e-05 9.9996591e-01]\n",
            "[9.9995625e-01 4.3778145e-05]\n",
            "[9.9972564e-01 2.7435028e-04]\n",
            "[3.1809046e-05 9.9996817e-01]\n",
            "[1.8017166e-05 9.9998200e-01]\n",
            "[9.9996972e-01 3.0258316e-05]\n",
            "[3.9951366e-05 9.9996006e-01]\n",
            "[3.1929212e-05 9.9996805e-01]\n",
            "[3.0509096e-05 9.9996948e-01]\n",
            "[9.9991500e-01 8.5031796e-05]\n",
            "[1.914443e-05 9.999808e-01]\n",
            "[9.9995613e-01 4.3821827e-05]\n",
            "[9.9989116e-01 1.0878897e-04]\n",
            "[2.4648163e-05 9.9997532e-01]\n",
            "[5.1366234e-05 9.9994862e-01]\n",
            "[2.0887583e-05 9.9997914e-01]\n",
            "[1.9573765e-05 9.9998045e-01]\n",
            "[5.0863287e-05 9.9994910e-01]\n",
            "[2.4009079e-05 9.9997604e-01]\n",
            "[2.6366995e-05 9.9997365e-01]\n",
            "[2.7480548e-05 9.9997246e-01]\n",
            "[9.9994743e-01 5.2546784e-05]\n",
            "[9.9993277e-01 6.7247391e-05]\n",
            "[2.3376468e-05 9.9997663e-01]\n",
            "[9.9993205e-01 6.7913752e-05]\n",
            "[1.1772741e-04 9.9988222e-01]\n",
            "[1.1756273e-04 9.9988246e-01]\n",
            "[2.4042141e-05 9.9997592e-01]\n",
            "[9.380929e-05 9.999062e-01]\n",
            "[3.4568162e-05 9.9996543e-01]\n",
            "[9.9995553e-01 4.4480916e-05]\n",
            "[2.4987334e-05 9.9997497e-01]\n",
            "[4.4552366e-05 9.9995542e-01]\n",
            "[2.4545163e-05 9.9997544e-01]\n",
            "[9.9997294e-01 2.7026557e-05]\n",
            "[4.266773e-05 9.999573e-01]\n",
            "[9.99898911e-01 1.01088095e-04]\n",
            "[4.1400806e-05 9.9995863e-01]\n",
            "[2.9991686e-05 9.9996996e-01]\n",
            "[1.9977464e-05 9.9997997e-01]\n",
            "[9.9975079e-01 2.4914913e-04]\n",
            "[2.2370423e-05 9.9997759e-01]\n",
            "[0.99891865 0.00108133]\n",
            "[5.4084067e-05 9.9994588e-01]\n",
            "[2.0316360e-05 9.9997973e-01]\n",
            "[1.8773451e-05 9.9998128e-01]\n",
            "[2.3317842e-05 9.9997663e-01]\n",
            "[9.2874703e-05 9.9990714e-01]\n",
            "[9.9994183e-01 5.8147634e-05]\n",
            "[9.9982721e-01 1.7284739e-04]\n",
            "[3.7249993e-05 9.9996281e-01]\n",
            "[2.318414e-05 9.999769e-01]\n",
            "[2.3123703e-05 9.9997687e-01]\n",
            "[0.9979377  0.00206237]\n",
            "[2.8910933e-04 9.9971086e-01]\n",
            "[2.5134936e-05 9.9997485e-01]\n",
            "[9.557628e-05 9.999044e-01]\n",
            "[4.728354e-05 9.999527e-01]\n",
            "[2.153239e-05 9.999784e-01]\n",
            "[3.8191440e-05 9.9996185e-01]\n",
            "[9.9990129e-01 9.8744924e-05]\n",
            "[3.095981e-05 9.999690e-01]\n",
            "[1.0966941e-04 9.9989033e-01]\n",
            "[2.831783e-05 9.999716e-01]\n",
            "[9.9994409e-01 5.5870747e-05]\n",
            "[2.8466531e-05 9.9997151e-01]\n",
            "[9.9994743e-01 5.2578667e-05]\n",
            "[9.9996865e-01 3.1330987e-05]\n",
            "[2.2235914e-05 9.9997771e-01]\n",
            "[9.9990833e-01 9.1606955e-05]\n",
            "[4.0906867e-05 9.9995911e-01]\n",
            "[9.9997497e-01 2.5046502e-05]\n",
            "[9.9995303e-01 4.7004487e-05]\n",
            "[2.4000612e-05 9.9997604e-01]\n",
            "[0.99856156 0.00143842]\n",
            "[5.5374006e-05 9.9994457e-01]\n",
            "[9.9997616e-01 2.3803350e-05]\n",
            "[9.9991155e-01 8.8422566e-05]\n",
            "[2.852376e-05 9.999715e-01]\n",
            "[3.9965664e-05 9.9996006e-01]\n",
            "[2.1732207e-05 9.9997830e-01]\n",
            "[4.9212558e-05 9.9995077e-01]\n",
            "[8.513240e-05 9.999149e-01]\n",
            "[3.0705589e-05 9.9996924e-01]\n",
            "[2.3373346e-05 9.9997663e-01]\n",
            "[0.99894506 0.00105498]\n",
            "[5.7164416e-05 9.9994278e-01]\n",
            "[6.130459e-05 9.999387e-01]\n",
            "[2.004829e-05 9.999800e-01]\n",
            "[0.9978981  0.00210182]\n",
            "[3.021947e-05 9.999697e-01]\n",
            "[9.9997091e-01 2.9132076e-05]\n",
            "[9.9996436e-01 3.5631809e-05]\n",
            "[3.1237341e-05 9.9996877e-01]\n",
            "[1.7965931e-05 9.9998200e-01]\n",
            "[8.1561629e-05 9.9991846e-01]\n",
            "[3.090766e-05 9.999691e-01]\n",
            "[9.9996912e-01 3.0852058e-05]\n",
            "[4.4415679e-05 9.9995553e-01]\n",
            "[3.4422166e-05 9.9996555e-01]\n",
            "[4.1596937e-05 9.9995840e-01]\n",
            "[3.7572041e-05 9.9996245e-01]\n",
            "[5.2380241e-05 9.9994767e-01]\n",
            "[0.43331423 0.5666858 ]\n",
            "[3.5266607e-05 9.9996471e-01]\n",
            "[2.0235844e-05 9.9997973e-01]\n",
            "[9.9997628e-01 2.3700151e-05]\n",
            "[2.2167571e-05 9.9997783e-01]\n",
            "[9.9967897e-01 3.2107698e-04]\n",
            "[4.0099050e-05 9.9995995e-01]\n",
            "[1.7852612e-05 9.9998212e-01]\n",
            "[1.6170823e-04 9.9983823e-01]\n",
            "[3.2401153e-05 9.9996758e-01]\n",
            "[4.2801803e-05 9.9995720e-01]\n",
            "[9.9997139e-01 2.8617544e-05]\n",
            "[9.999584e-01 4.154976e-05]\n",
            "[4.7013411e-05 9.9995303e-01]\n",
            "[6.2356048e-05 9.9993765e-01]\n",
            "[2.8460343e-05 9.9997151e-01]\n",
            "[9.999291e-01 7.093128e-05]\n",
            "[9.999615e-01 3.853948e-05]\n",
            "[2.5599789e-05 9.9997437e-01]\n",
            "[3.6906404e-05 9.9996305e-01]\n",
            "[0.19095193 0.8090481 ]\n",
            "[7.2298310e-05 9.9992764e-01]\n",
            "[0.95978177 0.04021824]\n",
            "[5.0442955e-05 9.9994957e-01]\n",
            "[5.1773630e-05 9.9994826e-01]\n",
            "[5.1742678e-05 9.9994826e-01]\n",
            "[1.6823324e-04 9.9983180e-01]\n",
            "[9.9990344e-01 9.6600030e-05]\n",
            "[0.33019948 0.6698005 ]\n",
            "[8.515855e-05 9.999149e-01]\n",
            "[3.9237773e-04 9.9960762e-01]\n",
            "[0.01720229 0.98279774]\n",
            "[2.7149392e-05 9.9997282e-01]\n",
            "[4.105839e-05 9.999590e-01]\n",
            "[9.998779e-01 1.221067e-04]\n",
            "[1.9348752e-05 9.9998069e-01]\n",
            "[2.6625637e-05 9.9997342e-01]\n",
            "[9.9997461e-01 2.5386047e-05]\n",
            "[9.9997234e-01 2.7682214e-05]\n",
            "[2.4836761e-05 9.9997520e-01]\n",
            "[2.354606e-05 9.999764e-01]\n",
            "[3.6365898e-05 9.9996364e-01]\n",
            "[3.443980e-04 9.996556e-01]\n",
            "[3.8527247e-05 9.9996150e-01]\n",
            "[1.7627723e-05 9.9998236e-01]\n",
            "[0.00208497 0.99791497]\n",
            "[2.6834765e-05 9.9997318e-01]\n",
            "[0.00492391 0.9950761 ]\n",
            "[3.4915305e-05 9.9996507e-01]\n",
            "[2.1733018e-05 9.9997830e-01]\n",
            "[3.2854558e-05 9.9996710e-01]\n",
            "[9.2772731e-05 9.9990726e-01]\n",
            "[9.9953103e-01 4.6900107e-04]\n",
            "[0.5438537  0.45614636]\n",
            "[1.9763715e-05 9.9998021e-01]\n",
            "[2.8321609e-05 9.9997163e-01]\n",
            "[7.611060e-05 9.999238e-01]\n",
            "[3.5500583e-05 9.9996448e-01]\n",
            "[0.96994317 0.03005686]\n",
            "[2.1949343e-05 9.9997807e-01]\n",
            "[9.999429e-01 5.707993e-05]\n",
            "[4.1458687e-05 9.9995852e-01]\n",
            "[2.5239287e-04 9.9974757e-01]\n",
            "[3.0711566e-05 9.9996924e-01]\n",
            "[9.9997032e-01 2.9691117e-05]\n",
            "[3.3133398e-05 9.9996686e-01]\n",
            "[3.0014720e-05 9.9996996e-01]\n",
            "[9.994287e-01 5.712721e-04]\n",
            "[3.2212112e-05 9.9996781e-01]\n",
            "[5.0717051e-04 9.9949276e-01]\n",
            "[0.00177328 0.9982267 ]\n",
            "[2.3444803e-05 9.9997652e-01]\n",
            "[0.99868447 0.00131548]\n",
            "[2.4700566e-05 9.9997532e-01]\n",
            "[2.8745446e-04 9.9971253e-01]\n",
            "[2.8602974e-05 9.9997139e-01]\n",
            "[5.9388654e-05 9.9994063e-01]\n",
            "[4.5759407e-05 9.9995422e-01]\n",
            "[0.05488104 0.9451189 ]\n",
            "[0.00114216 0.99885786]\n",
            "[2.1508769e-05 9.9997854e-01]\n",
            "[1.6410030e-04 9.9983585e-01]\n",
            "[3.0461588e-05 9.9996948e-01]\n",
            "[9.9994135e-01 5.8657562e-05]\n",
            "[2.4678196e-05 9.9997532e-01]\n",
            "[0.02213194 0.9778681 ]\n",
            "[1.8780868e-04 9.9981219e-01]\n",
            "[1.0532787e-04 9.9989462e-01]\n",
            "[0.00118718 0.9988128 ]\n",
            "[1.1940633e-04 9.9988055e-01]\n",
            "[2.5902387e-05 9.9997413e-01]\n",
            "[7.571196e-05 9.999243e-01]\n",
            "[0.9540573  0.04594264]\n",
            "[1.0802483e-04 9.9989200e-01]\n",
            "[9.9997354e-01 2.6440570e-05]\n",
            "[3.6754012e-05 9.9996328e-01]\n",
            "[2.1979591e-05 9.9997807e-01]\n",
            "[1.8523004e-05 9.9998152e-01]\n",
            "[3.0173973e-05 9.9996984e-01]\n",
            "[9.9993265e-01 6.7327332e-05]\n",
            "[2.2644132e-05 9.9997735e-01]\n",
            "[0.00373904 0.99626094]\n",
            "[9.9994051e-01 5.9516726e-05]\n",
            "[2.2177277e-05 9.9997783e-01]\n",
            "[0.8195864  0.18041363]\n",
            "[9.999651e-01 3.492260e-05]\n",
            "[2.725656e-05 9.999727e-01]\n",
            "[0.00342127 0.99657875]\n",
            "[1.6073146e-04 9.9983919e-01]\n",
            "[0.03266541 0.9673346 ]\n",
            "[2.9835532e-05 9.9997020e-01]\n",
            "[9.118957e-05 9.999088e-01]\n",
            "[0.08352035 0.91647965]\n",
            "[6.8121371e-05 9.9993193e-01]\n",
            "[5.4696044e-05 9.9994528e-01]\n",
            "[4.2533498e-05 9.9995744e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}