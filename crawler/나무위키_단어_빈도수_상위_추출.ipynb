{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "scrapingenv1",
   "display_name": "scrapingEnv1",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 나무위키 단어 빈도수 추출기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-213bc8a4e5e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mfinal_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mno_num_pattern_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mnouns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0munique_nouns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munique_nouns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\pydata\\scrapingenv1\\lib\\site-packages\\konlpy\\tag\\_hannanum.py\u001b[0m in \u001b[0;36mnouns\u001b[1;34m(self, phrase)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;34m\"\"\"Noun extractor.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'N'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\pydata\\scrapingenv1\\lib\\site-packages\\konlpy\\tag\\_hannanum.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, ntags, flatten, join)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mntags\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjhi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplePos09\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mntags\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjhi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplePos22\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "dir_path = './data_with_vling/txt_namuwiki/'\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "def preprocess_special(strings):\n",
    "    stop_special_char = re.sub(pattern='[가-힣 a-zA-Z\\d\\.\\n]+', repl='', string=strings)\n",
    "    stop_special_char_pattern = '[' + '\\\\'.join(set(stop_special_char)) + ']'\n",
    "    if stop_special_char_pattern == '[]':\n",
    "        return strings\n",
    "    new_strings = re.sub(pattern=stop_special_char_pattern, repl=' ',string=strings)\n",
    "    return new_strings\n",
    "def preprocess_no_eng_num(strings):\n",
    "    new_strings = re.sub(pattern='[a-zA-Z]',repl='',string=strings)\n",
    "    new_strings = re.sub(pattern='[0-9]', repl='', string=new_strings)\n",
    "    return new_strings\n",
    "def preprocess_no_num_pattern(strings):\n",
    "    new_strings = re.sub(pattern='[0-9]+[ㄱ-ㅎ가-힣]*', repl=' ', string=strings)\n",
    "    return new_strings\n",
    "\n",
    "stopwords = ['라', '중', '이', '때문', '지', '이상', '등', '수', '것', '시작', '부분', '당시', '경우', '이후', '오브', '리그', '편', '위', '정도', '활동', '전', '둘',\n",
    "    '한', '도중', '자체', '경기', '방송이','때','녹두','이유','문단을','대부분','번','일','리','두','초','나','문단','문','방송','적','후','단어','게임','유튜브','영상','말','역사','시청자들','본인','이전','시청자','업로드','콘텐츠','듯','현재','명','들','사람','1','개','관련','채널','모습','주','거','내','사이','자신','유튜버','컨텐츠','생각','내용','를','을','시즌','조','관','그','제목']    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, file_name in enumerate(file_list):\n",
    "    if i > 50:\n",
    "        break\n",
    "    with open(dir_path+file_name, 'rt', encoding='utf-8') as f:\n",
    "        origin_text = f.read()\n",
    "    no_special_text = preprocess_special(origin_text)\n",
    "    no_num_pattern_text = preprocess_no_num_pattern(no_special_text)\n",
    "\n",
    "    final_text = no_num_pattern_text\n",
    "    h = Hannanum()\n",
    "    nouns = h.nouns(final_text)\n",
    "    unique_nouns = set(nouns)\n",
    "    for word in unique_nouns:\n",
    "        if word in stopwords:\n",
    "            while word in nouns:\n",
    "                nouns.remove(word)\n",
    "\n",
    "    count = Counter(nouns)\n",
    "    top_20 = count.most_common(20)\n",
    "    print(type(top_20[0]))\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ]
}