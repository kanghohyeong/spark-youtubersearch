{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled23.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFzr4l64Sneb",
        "outputId": "ce964658-b4b7-4557-988e-eb01c84ee17e"
      },
      "source": [
        "!pip install konlpy\r\n",
        "from konlpy.tag import *\r\n",
        "komoran = Komoran()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 14.0MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 57.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: beautifulsoup4, tweepy, JPype1, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2NpWWt7S2U3",
        "outputId": "69c4f9fc-50fc-432b-c65d-05e66227c7a9"
      },
      "source": [
        "!pip install transformers boto3\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 4.2MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/3b/4da331f22280e6645c54c38b84d04f40858ae31298eecb1e3364c3add27e/boto3-1.17.3-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 60.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 60.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 45.1MB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/jmespath/\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/96/24f0e51870eb54701fa1b6f0249b31c3d1596c8e2fd9dcc5993ec9955266/botocore-1.20.3-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 57.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.3->boto3) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=0c3cf3ad0498ce401d2020581cb3f512bc7390ed4ef05a42d38b4a249a06121d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.3 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.17.3 botocore-1.20.3 jmespath-0.10.0 s3transfer-0.3.4 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0ImU_LS54N"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\r\n",
        "import torch\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from transformers import BertConfig , BertForSequenceClassification\r\n",
        "from transformers import BertTokenizer\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained('beomi/kcbert-base',num_labels=7)\r\n",
        "tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Exg6SxS772"
      },
      "source": [
        "import re\r\n",
        "emoji_pattern = re.compile(\"[\"\r\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "                           \"]+\", flags=re.UNICODE)\r\n",
        "only_BMP_pattern = re.compile(\"[\"\r\n",
        "        u\"\\U00010000-\\U0010FFFF\"  #BMP characters 이외\r\n",
        "                           \"]+\", flags=re.UNICODE)\r\n",
        "def split_token_get(text,tokenizer,MAX):\r\n",
        "  split_tokens = []\r\n",
        "  text = text.replace('\\n',' ')\r\n",
        "  text = emoji_pattern.sub(r'', text)\r\n",
        "  text = only_BMP_pattern.sub(r'', text)\r\n",
        "  \r\n",
        "  no_punctuation = re.split(r'([!,?]+)|([.]+)|([,]+)|([\"])|([\\'])|([&]+)|([(]+)|([)]+)|([~]+)|([♡]+)|([☆,★]+)',\r\n",
        "                              text.strip())\r\n",
        "  no_punctuation_text = []\r\n",
        "\r\n",
        "  for string in no_punctuation:\r\n",
        "      if (string == '') or (string is None): continue\r\n",
        "      no_punctuation_text.append(string)\r\n",
        "\r\n",
        "  no_punctuation_text = ' '.join(no_punctuation_text)\r\n",
        "\r\n",
        "  split_char = re.split(r'([ㄱ-ㅣ0-9]+)', no_punctuation_text.strip())\r\n",
        "  split_char = ' '.join(split_char)\r\n",
        "  split_char = re.split(r'([ㅎ]{2,})|([ㅜ,ㅠ]{2,})|([ㅗ]+)|([ㅋ,ㄱ,ㄲ]{2,})|\\s+', split_char.strip())\r\n",
        "  final_text = []\r\n",
        "  for string in split_char:\r\n",
        "        if (string == '') or (string is None): continue\r\n",
        "        final_text.append(string)\r\n",
        "    \r\n",
        "  for token in komoran.morphs(text):\r\n",
        "    for sub_token in tokenizer.tokenize(token):\r\n",
        "      split_tokens.append(sub_token)\r\n",
        "  if len(split_tokens) > MAX-2:\r\n",
        "    split_tokens = split_tokens[:(MAX-2)]\r\n",
        "  split_tokens = [\"[CLS]\"]+ split_tokens + [\"[SEP]\"]\r\n",
        "  return split_tokens\r\n",
        "def get_input_ids(token , tokenizer , MAX):\r\n",
        "  segment_ids = [0] * len(token)\r\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(token)\r\n",
        "\r\n",
        "  input_mask = [1] * (len(input_ids) -1 )\r\n",
        "  input_mask += [0]\r\n",
        "  \r\n",
        "  padding = [0] * (MAX -len(input_ids) )\r\n",
        "\r\n",
        "  input_ids += padding\r\n",
        "  input_mask += padding\r\n",
        "  segment_ids += padding\r\n",
        "\r\n",
        "  assert len(input_ids) == MAX\r\n",
        "  return input_ids \r\n",
        "def get_segment_ids(token, tokenizer, MAX):\r\n",
        "  segment_ids = [0] * len(token)\r\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(token)\r\n",
        "  padding = [0] * (MAX -len(input_ids) )\r\n",
        "  \r\n",
        "  segment_ids += padding\r\n",
        "  assert len(segment_ids) == MAX\r\n",
        "  return segment_ids \r\n",
        "def get_input_mask(token, tokenizer, MAX):\r\n",
        "  \r\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(token)\r\n",
        "\r\n",
        "  input_mask = [1]* (len(input_ids) -1 )\r\n",
        "  input_mask += [0]\r\n",
        "\r\n",
        "  padding = [0] * (MAX -len(input_ids) )\r\n",
        "\r\n",
        "  input_mask += padding\r\n",
        "\r\n",
        "  assert len(input_mask) == MAX\r\n",
        "  return input_mask"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YH0WGiSTCy8"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "data = pd.read_csv(\"labeled.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6FA9n4bTO1g"
      },
      "source": [
        "for i in range(20):\r\n",
        "  data = data.sample(frac=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyxTv4_qTZdJ"
      },
      "source": [
        "sentences = data['Sentence']\r\n",
        "labels = data['label'].values\r\n",
        "\r\n",
        "\r\n",
        "sentences1 = sentences[32804:]\r\n",
        "labels1 = labels[32804:]\r\n",
        "\r\n",
        "\r\n",
        "sentences = sentences[:32804]\r\n",
        "labels = labels[:32804]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYZ4QLlqTuYb"
      },
      "source": [
        "tokenized_texts = [split_token_get(ss, tokenizer,128) for ss in sentences]\r\n",
        "input_ids = [get_input_ids(ss,tokenizer,128)for ss in tokenized_texts]\r\n",
        "segment_ids = [get_segment_ids(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "input_mask = [get_input_mask(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "\r\n",
        "input_ids = torch.tensor(input_ids)\r\n",
        "segment_ids = torch.tensor(segment_ids)\r\n",
        "input_mask = torch.tensor(input_mask)\r\n",
        "labels = torch.tensor(labels,dtype = torch.long)\r\n",
        "\r\n",
        "train_data = TensorDataset(input_ids,input_mask,labels)\r\n",
        "train_dataloader = DataLoader(train_data, batch_size = 32)\r\n",
        "\r\n",
        "\r\n",
        "tokenized_texts = [split_token_get(ss, tokenizer,128) for ss in sentences1]\r\n",
        "input_ids = [get_input_ids(ss,tokenizer,128)for ss in tokenized_texts]\r\n",
        "segment_ids = [get_segment_ids(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "input_mask = [get_input_mask(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "\r\n",
        "input_ids = torch.tensor(input_ids)\r\n",
        "segment_ids = torch.tensor(segment_ids)\r\n",
        "input_mask = torch.tensor(input_mask)\r\n",
        "labels = torch.tensor(labels1,dtype = torch.long)\r\n",
        "\r\n",
        "train_data1 = TensorDataset(input_ids,input_mask,labels)\r\n",
        "train_dataloader1 = DataLoader(train_data1, batch_size = 8)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qq0f0d3TyEM"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')\r\n",
        "if torch.cuda.is_available():    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "else:\r\n",
        "    device = torch.device(\"cpu\")\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FVfQKJHT0Ew"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    \r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\r\n",
        "def format_time(elapsed):\r\n",
        "    # 반올림\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # hh:mm:ss으로 형태 변경\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8LCrjJMT2Er"
      },
      "source": [
        "from transformers import AdamW\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  lr = 2e-5,\r\n",
        "                  eps = 1e-8)\r\n",
        "epochs = 5\r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps = 0,\r\n",
        "                                            num_training_steps = total_steps)\r\n",
        "\r\n",
        "seed_val = 42\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "\r\n",
        "model.zero_grad()\r\n",
        "eval_accuracy = 0\r\n",
        "nb_eval_steps = 0\r\n",
        "for i in range(0, epochs):\r\n",
        "  optimizer.zero_grad()\r\n",
        "  t0 = time.time()\r\n",
        "  total_loss = 0\r\n",
        "  model.train()\r\n",
        "  eval_accuracy = 0\r\n",
        "  nb_eval_steps = 0\r\n",
        "  for step,batch in enumerate(train_dataloader):\r\n",
        "    model.train()\r\n",
        "    if step%100 == 0 and not step ==0:\r\n",
        "      elapsed = format_time(time.time() - t0)\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed) )\r\n",
        "    \r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "    input_ids , input_mask , labels = batch\r\n",
        "\r\n",
        "    outputs = model(input_ids= input_ids,\r\n",
        "                    token_type_ids = None,\r\n",
        "                    attention_mask=None,\r\n",
        "                    labels = labels)\r\n",
        "    loss = outputs\r\n",
        "\r\n",
        "    total_loss += loss[0].item()\r\n",
        "\r\n",
        "    loss[0].backward()\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "  \r\n",
        "    optimizer.step()\r\n",
        "    scheduler.step()\r\n",
        "\r\n",
        "    model.zero_grad()\r\n",
        "  avg_train_loss = total_loss / len(train_dataloader)            \r\n",
        "\r\n",
        "  print(\"\")\r\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "  print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "  \r\n",
        "  t0 = time.time()\r\n",
        "  model.eval()\r\n",
        "  \r\n",
        "  eval_accuracy = 0\r\n",
        "  nb_eval_steps = 0\r\n",
        "  for step,batch in enumerate(train_dataloader1):\r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "\r\n",
        "    input_ids , input_mask, labels = batch\r\n",
        "    outputs = model(input_ids = input_ids ,\r\n",
        "                    token_type_ids = None , \r\n",
        "                    attention_mask = input_mask \r\n",
        "                    )\r\n",
        "    logits = outputs[0]\r\n",
        "\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "    labels = labels.to('cpu').numpy()\r\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, labels)\r\n",
        "    eval_accuracy += tmp_eval_accuracy\r\n",
        "    nb_eval_steps += 1\r\n",
        "  print(\"예측 Accuracy: {0:.5f}\".format(eval_accuracy/nb_eval_steps))\r\n",
        "  print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOBfeGnYT-Q_"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/My Drive/today2.pt')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNbAoGjW4tDI"
      },
      "source": [
        "data = pd.read_csv(\"data.csv\", encoding = 'cp949')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z37r9WQW418H"
      },
      "source": [
        "k = []\r\n",
        "for i in range(0,100):\r\n",
        "  name = '/content/youtube0_100/'\r\n",
        "  path = ''\r\n",
        "  dat = pd.read_csv(path+name)\r\n",
        "  del dat['0']\r\n",
        "  dat= dat.dropna(axis = 0)\r\n",
        "\r\n",
        "  sss = dat['comment']\r\n",
        "  tokenized_texts = [split_token_get(ss, tokenizer,128) for ss in sss]\r\n",
        "  input_ids = [get_input_ids(ss,tokenizer,128)for ss in tokenized_texts]\r\n",
        "  segment_ids = [get_segment_ids(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "  input_mask = [get_input_mask(ss,tokenizer,128) for ss in tokenized_texts]\r\n",
        "\r\n",
        "  input_ids = torch.tensor(input_ids)\r\n",
        "  segment_ids = torch.tensor(segment_ids)\r\n",
        "  input_mask = torch.tensor(input_mask)\r\n",
        "\r\n",
        "  train_data1 = TensorDataset(input_ids,input_mask)\r\n",
        "  train_dataloader1 = DataLoader(train_data1, batch_size = 8)\r\n",
        "  lab = [0,0,0,0,0,0,0]\r\n",
        "\r\n",
        "  for step,batch in enumerate(train_dataloader1):\r\n",
        "    \r\n",
        "    batch = tuple(t.to(device) for t in batch)\r\n",
        "\r\n",
        "    input_ids , input_mask = batch\r\n",
        "    outputs = model(input_ids = input_ids ,\r\n",
        "                    token_type_ids = None , \r\n",
        "                    attention_mask = input_mask \r\n",
        "                    )\r\n",
        "    logits = outputs[0]\r\n",
        "    for j in range(len(logits)):\r\n",
        "      k.append(list(logits[j]).index(logits[j].max()))\r\n",
        "  '''\r\n",
        "    for j in range(len(logits)):\r\n",
        "      num1 = list(logits[j]).index(logits[j].max())\r\n",
        "      lab[num1] += 1\r\n",
        "\r\n",
        "  ls[i] = lab\r\n",
        "  print(lab)\r\n",
        "  '''"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj9qrtLk-W4P"
      },
      "source": [
        "dat['label'] =k"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlWFYom7CBOq"
      },
      "source": [
        "df = pd.DataFrame({'name':data['name'],'youtube_code':data['youtube_code'],'namu_code':data['namu_code'],\r\n",
        "                   '0':np.array(ls).T[0],\r\n",
        "                   '1':np.array(ls).T[1],\r\n",
        "                   '2':np.array(ls).T[2],\r\n",
        "                   '3':np.array(ls).T[3],\r\n",
        "                   '4':np.array(ls).T[4],\r\n",
        "                   '5':np.array(ls).T[5],\r\n",
        "                   '6':np.array(ls).T[6]})"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWlOb9ZX7-jw"
      },
      "source": [
        "df.to_csv(\"d.csv\")"
      ],
      "execution_count": 165,
      "outputs": []
    }
  ]
}