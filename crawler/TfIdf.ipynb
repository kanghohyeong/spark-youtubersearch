{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "scrapingenv1",
   "display_name": "scrapingEnv1",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tf - Idf 구현"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 하나 열어서 전처리 후 단어 추출 후 다시 문장으로 만드는 함수\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "def make_clean_sting(filename):\n",
    "    # 파일 읽기\n",
    "    with open(filename, mode='rt', encoding='utf-8') as f:\n",
    "        strings = f.read()\n",
    "    \n",
    "    #전처리\n",
    "    stop_special_char = re.sub(pattern='[가-힣 a-zA-Z\\d\\.\\n]+', repl='', string=strings)\n",
    "    stop_special_char_pattern = '[' + '\\\\'.join(set(stop_special_char)) + ']'\n",
    "\n",
    "    new_strings = re.sub(pattern=stop_special_char_pattern, repl='',string=strings)\n",
    "    new_strings = re.sub(pattern='\\s{2,}',repl=' ',string=new_strings)\n",
    "    new_strings = re.sub(pattern='[\\n]+',repl=' ',string=new_strings)\n",
    "    new_strings = re.sub(pattern='[a-zA-Z\\,]+',repl='',string=new_strings)\n",
    "    new_strings = re.sub(pattern='[0-9]+', repl='', string=new_strings)\n",
    "    new_strings = re.sub(pattern='\\s{2,}',repl=' ',string=new_strings)\n",
    "    new_strings = re.sub(pattern='롤', repl='리그오브레전드', string=new_strings)\n",
    "    new_strings = re.sub(pattern='아프리카', repl='아프리카티비', string=new_strings)\n",
    "    new_strings = re.sub(pattern='아프리카티비티비', repl='아프리카티비', string=new_strings)\n",
    "\n",
    "    # 명사 추출\n",
    "    hannanum = Hannanum()\n",
    "    string2noun = hannanum.nouns(new_strings)\n",
    "    \n",
    "    #다시 합치기\n",
    "    noun2string = \" \".join(string2noun)\n",
    "\n",
    "\n",
    "    print('task [%s] clear' % filename)\n",
    "    return noun2string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc set 만들기\n",
    "\n",
    "gamst = make_clean_sting('./crawling_result_text/gamst_namu.txt')\n",
    "handongsuk = make_clean_sting('./crawling_result_text/handongsuk_namu.txt')\n",
    "testerhoon = make_clean_sting('./crawling_result_text/testerhoon_namu.txt')\n",
    "\n",
    "gamst = gamst + make_clean_sting('./crawling_result_text/gamst2.txt')\n",
    "handongsuk = handongsuk + make_clean_sting('./crawling_result_text/handongsuk2.txt')\n",
    "testerhoon = testerhoon + make_clean_sting('./crawling_result_text/testerhoon2.txt')\n",
    "\n",
    "print(gamst, handongsuk, testerhoon)\n",
    "\n",
    "doc_list = []\n",
    "doc_list.append(gamst)\n",
    "doc_list.append(handongsuk)\n",
    "doc_list.append(testerhoon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "x = tfidf.fit_transform(doc_list)\n",
    "word_list = tfidf.get_feature_names()\n",
    "# print(x)\n",
    "# print(x.data)\n",
    "# print(x.indices)\n",
    "# print(x.indptr)\n",
    "\n",
    "\n",
    "new_data_list = []\n",
    "\n",
    "for i in range(0,x.indptr[3]):\n",
    "    new_data = [0,0]\n",
    "    new_data[0] = x.data[i]\n",
    "    new_data[1] = x.indices[i]\n",
    "    new_data_list.append(new_data)\n",
    "\n",
    "top10_list = [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "\n",
    "\n",
    "#첫번째 문서 탑10\n",
    "for i in range(0,x.indptr[1]):\n",
    "    current_value = new_data_list[i][0]\n",
    "    if top10_list[19][0] < current_value:\n",
    "        top10_list[19] = new_data_list[i]\n",
    "        top10_list.sort(key=lambda x:x[0],reverse=True)\n",
    "        \n",
    "\n",
    "for i in top10_list:\n",
    "    print(word_list[i[1]])\n",
    "\n",
    "top10_list = [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "print('\\n')\n",
    "#두번째 문서 탑10\n",
    "for i in range(x.indptr[1],x.indptr[2]):\n",
    "    current_value = new_data_list[i][0]\n",
    "    if top10_list[19][0] < current_value:\n",
    "        top10_list[19] = new_data_list[i]\n",
    "        top10_list.sort(key=lambda x:x[0],reverse=True)\n",
    "        \n",
    "\n",
    "for i in top10_list:\n",
    "    print(word_list[i[1]])\n",
    "\n",
    "top10_list = [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "print('\\n')\n",
    "#세번째 문서 탑10\n",
    "for i in range(x.indptr[2],x.indptr[3]):\n",
    "    current_value = new_data_list[i][0]\n",
    "    if top10_list[19][0] < current_value:\n",
    "        top10_list[19] = new_data_list[i]\n",
    "        top10_list.sort(key=lambda x:x[0],reverse=True)\n",
    "        \n",
    "\n",
    "for i in top10_list:\n",
    "    print(word_list[i[1]])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}